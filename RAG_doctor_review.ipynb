{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval-augmented Generation (RAG) with german doctor ratings  \n",
        "\n",
        "## Ingesting and retrieving reviews with **llama-index** and **Huggingface**\n",
        "In this notebook we will be looking at the ingestion and retrieval process of a Retrieval-augmented Generation system.\n",
        "\n",
        "You will build the ingestion pipeline to read in german doctor reviews. Along the way you will compute text embeddings, store the information into Vector-Database and setup the components to perform the retrieval of relevant information for a user query.\n",
        "\n",
        "To faciliate the data transformation we will rely on the [**LlamaIndex**](https://docs.llamaindex.ai/) package and make use of [**HuggingFace's Inference API**](https://huggingface.co/docs/api-inference/en/index)"
      ],
      "metadata": {
        "id": "1wK1bBSZ0cka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index==0.12.24 llama-index-vector-stores-chroma==0.4.1 llama-index-llms-huggingface-api==0.4.1 llama-index-embeddings-huggingface-api==0.3.0"
      ],
      "metadata": {
        "id": "sDXhSCCcT_j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wqEOEJvGUHjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "from llama_index.embeddings.huggingface_api import HuggingFaceInferenceAPIEmbedding\n",
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "import chromadb\n",
        "import nest_asyncio\n",
        "\n",
        "tqdm.tqdm.pandas()\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "pd.options.display.max_colwidth = 600\n",
        "pd.options.display.max_rows = 400"
      ],
      "metadata": {
        "id": "aOT9HkAIVnDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Huggingface\n",
        "To be able to access the Huggingface Inference API we will have to use an access token.\n",
        "\n",
        "You will need your Huggingface account. If you not have done already, perform the sign up on https://huggingface.co\n",
        "\n",
        "Then create a fresh access token under https://huggingface.co/settings/tokens\n",
        "Make sure to\n",
        "- select \"Fine-grained\" as token type\n",
        "- give the token a name and\n",
        "- tick under `Inference`\n",
        "  - `Make calls to inference providers` and\n",
        "  - `Make calls to Inference Endpoints`\n",
        "- Store the generated access token somewhere safely\n",
        "\n",
        "Finally, execute the cell bellow and copy&paste the token into the provided field to configure the access of this notebook."
      ],
      "metadata": {
        "id": "ZlD1TUWv2rQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ],
      "metadata": {
        "id": "fqp_Nk3qUMoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the **German language reviews of doctors by patients 2019** dataset"
      ],
      "metadata": {
        "id": "Ry8K9t3q5Ycw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O reviews.zip https://query.data.world/s/v5xl53bs2rgq476vqy7cg7xx2db55y\n",
        "!unzip reviews.zip"
      ],
      "metadata": {
        "id": "PZ42T7bhUuB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data loading\n",
        "The dataset contains patients' text comments in german and a coresponding ratings.\n",
        "\n",
        "We will intermediately load the data into a Pandas DataFrame. Then, to ease the loading process with LlamaIndex, write each review into a separate file.\n",
        "\n",
        "We limit ourselfs to the first 10 entries of the dataset not to overuse available credits of the Huggingface Inference API."
      ],
      "metadata": {
        "id": "CDIEIh-d5joX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read data from csv\n",
        "reviews = pd.read_csv(\"german_doctor_reviews.csv\")[:10]"
      ],
      "metadata": {
        "id": "uKinMlJGVY95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the target folder for the reviews\n",
        "dataset_path = Path('german_doctor_reviews')\n",
        "\n",
        "if dataset_path.exists():\n",
        "    shutil.rmtree(dataset_path)\n",
        "\n",
        "dataset_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# write one file per review\n",
        "for idx, row in tqdm.tqdm(reviews.iterrows(), total=len(reviews)):\n",
        "    with (dataset_path / f\"review_{idx}.txt\").open(\"w\") as fp:\n",
        "        fp.write(row[\"comment\"])"
      ],
      "metadata": {
        "id": "oeUXILjnVe7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QtMBBi78WtKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the documents\n",
        "Now that the data is properly stored on the file system we can leverage LlamaIndex' `SimpleDirectoryReader` to skim through the directory and load each document into memory"
      ],
      "metadata": {
        "id": "b3aU8UYx6zMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reader = SimpleDirectoryReader(input_dir=str(dataset_path))\n",
        "documents = reader.load_data()\n",
        "len(documents)"
      ],
      "metadata": {
        "id": "q-jBorTMWqWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a6WL_YmRXURo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ingesting the document into a vector store"
      ],
      "metadata": {
        "id": "0FyzOYL67WNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the vector store\n",
        "We will be using [`chroma` ](https://github.com/chroma-core/chroma) as our embedding or vector database. It natively supports embeddings and vector similarity searches and can very easily be integrated into the common GenAI orchestration frameworks."
      ],
      "metadata": {
        "id": "2h04sfL57hH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "chroma_collection = db.get_or_create_collection(name=\"german_doctor_reviews\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
      ],
      "metadata": {
        "id": "z19WquiY-3R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PXKhD_e1AlDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the embedding model\n",
        "Experimenting with different embedding models is made very easy by using Huggingface Inference API. Popular models are hosted on their infrastructure.\n",
        "\n",
        "For our case we will be using [`BAAI/bge-small-en-v1.5`](https://huggingface.co/BAAI/bge-small-en-v1.5) an embedding model trained and published by the Beijing Academy of Artificial Intelligence.\n",
        "\n",
        "The BGE (BAAI General Embedding) focuses on retrieval-augmented LLMs"
      ],
      "metadata": {
        "id": "pTjwAZwD-90s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = HuggingFaceInferenceAPIEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ],
      "metadata": {
        "id": "zFWBZ64S-7lD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the pipeline\n",
        "Now its time to put everything together: LlamaIndex' `IngestionPipeline` allows us to defeine transformation steps and the target vector store where the ingested data should be stored.\n",
        "\n",
        "Our pipeline will use a `SentenceSplitter` to chunk up the different reviews by sentence. Then the pipeline will pass each chunk to our embedding model to obtain a vector representation of the text.\n",
        "Lastly, the pipeline will load each chunk along with its embedding into the chroma vector store."
      ],
      "metadata": {
        "id": "mzYIYqwDAg9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        SentenceSplitter(),\n",
        "        embed_model,\n",
        "    ],\n",
        "    vector_store=vector_store,\n",
        ")\n"
      ],
      "metadata": {
        "id": "kEa0ERqbAfx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Indexing the document\n",
        "We are all set now! Lets execute the pipeline and ingest all our reviews into the vector database"
      ],
      "metadata": {
        "id": "_nrxPzG37_UL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = await pipeline.arun(documents=documents)\n",
        "len(nodes)"
      ],
      "metadata": {
        "id": "zk4A9kyKX0td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data analysis\n",
        "The pipeline processed all the documents and returned the nodes that were created along the process.\n",
        "\n",
        "Lets explore the returned data in more detail"
      ],
      "metadata": {
        "id": "plhiuD3x8Vr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "## YOUR CODE HERE START ##\n",
        "##########################\n",
        "\n",
        "# Have a look at the returned nodes.\n",
        "# Print the text and its corresponding embedding of the first node\n",
        "\n",
        "print(....)\n",
        "\n",
        "##########################\n",
        "## YOUR CODE HERE END ##\n",
        "##########################"
      ],
      "metadata": {
        "id": "OGH_VHjv7NYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get a better feeling what the embeddings represent we can compute the cosine distance between all the embedding vectors.\n",
        "\n",
        "Cosine distance is a measure of dissimilarity between two vectors in an inner product space. It is derived from the **cosine similarity**, which computes the cosine of the angle between two vectors. T\n",
        "\n",
        "The cosine distance ranges from **0** (when vectors are identical) to **2** (when vectors point in exactly opposite directions)."
      ],
      "metadata": {
        "id": "f-OPVwBU85eG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_distances"
      ],
      "metadata": {
        "id": "rdSyt4h2YQHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = [node.metadata[\"file_name\"] for node in nodes]\n",
        "\n",
        "##########################\n",
        "## YOUR CODE HERE START ##\n",
        "##########################\n",
        "\n",
        "# compute the distances for each embedding to each other embedding\n",
        "\n",
        "embeddings = [....]\n",
        "distances = ....\n",
        "\n",
        "##########################\n",
        "## YOUR CODE HERE END ##\n",
        "##########################\n",
        "\n",
        "plt.imshow(distances, interpolation='nearest', cmap='viridis')\n",
        "plt.colorbar(label='Cosine Distance')\n",
        "\n",
        "plt.xticks(ticks=np.arange(len(filenames)), labels=filenames, rotation=90)\n",
        "plt.yticks(ticks=np.arange(len(filenames)), labels=filenames)\n",
        "plt.title('Cosine Distance Heatmap between Text Embeddings')"
      ],
      "metadata": {
        "id": "C2tct5YnymgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which reviews are similiar, which are very different?\n",
        "\n",
        "Can you see a pattern in the data?"
      ],
      "metadata": {
        "id": "nOF_h2RARMPB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xWkwSVjq-VLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval-augmented Generation in action\n",
        "`LlamaIndex` allows us to turn our vector database into an index that we can query with a string.\n",
        "\n",
        "We will need to define the `VectorStoreIndex` using our previously created `vector_store` and also pass along our `embed_model`. This will allow the index to turn a user query into an embedding an search for the most similar entries.\n",
        "\n",
        "These entries will then subsequently be used to formulate an answer to the user inquery."
      ],
      "metadata": {
        "id": "WxjXhaLR-W5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nest_asyncio.apply()  # This is needed to run the query engine\n",
        "\n",
        "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
        "\n",
        "index = VectorStoreIndex.from_vector_store(\n",
        "    vector_store=vector_store, embed_model=embed_model\n",
        ")\n",
        "query_engine = index.as_query_engine(\n",
        "    llm=llm,\n",
        "    response_mode=\"tree_summarize\",\n",
        ")"
      ],
      "metadata": {
        "id": "2VRkp9U132V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"Welcher Doktor ist gut in München für Zahnprobleme?\"\n",
        "\n",
        "##########################\n",
        "## YOUR CODE HERE START ##\n",
        "##########################\n",
        "\n",
        "# Use the query_engine to answer the user query\n",
        "\n",
        "response = ....\n",
        "\n",
        "##########################\n",
        "## YOUR CODE HERE END ##\n",
        "##########################\n",
        "\n",
        "\n",
        "print('Query:', user_query)\n",
        "print('Response:', response.response)\n",
        "print('References:')\n",
        "for node in response.source_nodes:\n",
        "    print(f'{node.metadata[\"file_name\"]} -{node.text[:200]}')"
      ],
      "metadata": {
        "id": "XYyKrkiZ4qJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rNkbRGIBAb9K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}